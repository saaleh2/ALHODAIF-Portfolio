{
  
    
        "post0": {
            "title": "Building Excel dashboard using NYSE data",
            "content": "This project is from my Udacityies&#39; &quot;Business Analysis&quot; nanodegree last year. Udacity has famously known for thair project-based courses. Meaning that candidates can only get thair certificate if they apply what they have learned in real-life projects. Candidates&#39; projects get checked by experts to evaluate their work and make sure they have fulfilled Udacityies&#39; requirements. . This project uses the NYSE dataset for over 450 companies. It includes companies&#39; financials data for four years, period date, sector, and industry. Here&#39;s a look at the dataset head: . To pass this project I&#39;ve been asked to attain two requirements: . Find insights from the data and tell a story about it through a presentation. | Build a dynamic financial dashboard in excel. | . Find insights in the data . The dataset includes financial data from 2012 to 2016. The first thing that came to my mind of that period is when the oil price hit 120$ in 2012 and then fell in 2015. I was wondering how the airline industry did during that period. Why airline? because during that time I was studying in China and I&#39;ve noticed that airline ticket prices were getting more and more expensive. So I wanted to see if there&#39;s a correlation between ticket prices and oil prices to confirm my hypothesis. In general, if oil prices go up or down it affects many aspects of the global economy, some sectors benefit from high prices but most of them benefit from lower prices. my question was &quot;how was airline companies&#39; financial performance during that period?&quot; My assumption was that high oil prices will increase the cost of airline operations, which therefore increases the price of tickets. High ticket prices lead to lower demand and therefore lower profits. . Extracting EBIT from data . The main benchmarks I used to answer my question are total revenue and EBIT (earnings before interest and tax). There are other factors that could tell you about companies&#39; performance, but these two are good for my question. We don&#39;t have EBIT in the dataset, But luckily we have the raw data to extract EBIT. To do that, First I found the Gross Profit by subtracting Cost of Goods Sold from Total Revenue then we get EBIT by subtracting Sales, General and Admin from Gross Profit. Lastly, I used the wonderful pivot table tool, to get average, median,, and standard deviation of the two benchmarks mentioned earlier. Using them all together will give us more accurate insight. . Here&#39;s the result on excel: . Average EBIT &amp; revenue &emsp; | . . &emsp; . Median EBIT &emsp; | . . &emsp; . EBIT standard deviation &emsp; | . . &emsp; . Here are my insights in clean slides: . Use the full-screen button in the lower right corner. . . Building dynamic dashboard in Excel . Udacity required me to build two dynamic dashboards: . P/L (Profit and loss) dashboard. | Forecast analysis dashboard with three case scenarios. | . A dynamic dashboard means that the user can choose the company symbol and read P/L or predictions for any company individually. The prediction dashboard predicts how a company would perform in the next two years. . P/L statment dashboard . This dashboard is simple, I just brought the data from the dataset sheet into each cell using INDEX and MATCH functions and used Ctrl+Shift+Enter to turn it into an array formula. Try it yourself: &emsp; . &emsp; . Forecast dashboard . This dashboard is different. Here I&#39;m required to build a dynamic dashboard that can show each company forecast with: . Three scenarios: week, base, and strong scenario. | Operating scenarios | . First, I created the ratios table like Gross margin and Revenue growth percentages because assumptions will be extracted from past years&#39; ratios. Then under that table, I created the operating scenario table (sensitivity analysis). I could&#39;ve implemented this table in the final formula but this will not allow the users to read ratios when they need it. Finally, I built the assumption table with past data as a final result. In all tables, I used INDEX, OFFSET, and MATCH but in a boolean way. This is an example of a formula from one of the cells: . {=INDEX(total_revenue,MATCH(1,($F$5=symbols)*(G$8=years),0))} . &emsp; This is the forecasting dashboard, give it a try. &emsp; . &emsp; &emsp; If you would like to play with the file yourself Click here to open the full file on OneDrive. If you have any question please contact me on my LinkedIn or Twitter .",
            "url": "https://saaleh2.github.io/ALHODAIF-Portfolio/excel/dashboards/2022/02/12/NYSE-data-analysis.html",
            "relUrl": "/excel/dashboards/2022/02/12/NYSE-data-analysis.html",
            "date": " • Feb 12, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Online Store Visitors Behavioral Analysis",
            "content": "A while ago, I found this dataset on Kaggle which contains 20M user events from an online store. Its size over two Gigabite and the owner challenged data scientists to dig and find insights. so I took this challenge. the dataset has been separated into five files, each file contains events for an entire month. Starting from Octobor 2019 to Febrary 2020. . . Note: If you are a technical girl/gey click here to skip to the analysis. . For non-technicals . Before starting this analysis I will explain some technical terms so they become easy to digest while reading this post. . events record First of all, what is event?&emsp;an event is a recod of user actions in apps and websites. Each click you do in apps and websites will be recorded for future analysis and other purposes. The following example is simplifying how events recorded in a databases: | . event time event type product id category id brand price user id user session . 1 2019/1/1 - 8:00 PM | add to cart | milk | food | Almarai | 10.5 | saleh | 1115 | . 2 2019/1/1 - 8:05 PM | purchase | milk | food | Almarai | 10.5 | saleh | 1115 | . 3 2019/1/2 - 1:00 AM | view item | Iphone 13 | electronics | Apple | 1499.0 | salman | 1125 | . 4 2019/1/2 - 1:10 AM | add to cart | Iphone 13 | electronics | Apple | 1499.0 | salman | 1125 | . 5 2019/1/3 - 11:00 AM | view item | soap | cleaners | NaN | 15.0 | saleh | 2334 | . How to read the events data? The event time column records when did the event happen. And the event type column specify what the user did. product id, category id, and user id columns are always represented in numbers, but here we used text instead to simplify it. It&#39;s easier for computers to use numbers rather than texts especially when there are duplicates, like when two users have the same name. Sessions Before explaining user session column, let&#39;s clear out what a session means. A session is a group of user interactions with the website or app that take place within 30 minutes. So let&#39;s say you opened Noon app at 8:00 PM and start viewing items until 8:29 PM. All events that have been recorded during this period will be given a single session reference number. And when you open Noon again the next day it will be considered as a new session and a new reference number will take a palce. Notice how the user id &quot;saleh&quot; got a new user session reference number after coming back on 2019/1/3. | . Empty values . If you look back at row number 5 you will see that brand cells are NaN. Nan is actually not a brand, it simply means empty cell. Sometimes you will see null instead but they are the same thing. . . &nbsp; . Let us explore our data . We&#39;re going first to look at our data structure and prepare the data for analysis. . . Tip: Non-technicals can skip codes in the black boxes and read outputs (results) of the code under it. . # importing all necessary libraries for this analysis. import pandas as pd import numpy as np import plotly.express as px # Reading the data and taking a quick look at data structure. oct_file = pd.read_csv(r&#39;D: saleh kaggle 2019-Oct.csv&#39;) nov_file = pd.read_csv(r&#39;D: saleh kaggle 2019-Nov.csv&#39;) dec_file = pd.read_csv(r&#39;D: saleh kaggle 2019-Dec.csv&#39;) jan_file = pd.read_csv(r&#39;D: saleh kaggle 2020-Jan.csv&#39;) feb_file = pd.read_csv(r&#39;D: saleh kaggle 2020-Feb.csv&#39;) dataset = pd.concat([oct_file, nov_file, dec_file, jan_file, feb_file], ignore_index=True) dataset.head() . . event_time event_type product_id category_id category_code brand price user_id user_session . 0 2019-10-01 00:00:00 UTC | cart | 5773203 | 1487580005134238553 | NaN | runail | 2.62 | 463240011 | 26dd6e6e-4dac-4778-8d2c-92e149dab885 | . 1 2019-10-01 00:00:03 UTC | cart | 5773353 | 1487580005134238553 | NaN | runail | 2.62 | 463240011 | 26dd6e6e-4dac-4778-8d2c-92e149dab885 | . 2 2019-10-01 00:00:07 UTC | cart | 5881589 | 2151191071051219817 | NaN | lovely | 13.48 | 429681830 | 49e8d843-adf3-428b-a2c3-fe8bc6a307c9 | . 3 2019-10-01 00:00:07 UTC | cart | 5723490 | 1487580005134238553 | NaN | runail | 2.62 | 463240011 | 26dd6e6e-4dac-4778-8d2c-92e149dab885 | . 4 2019-10-01 00:00:15 UTC | cart | 5881449 | 1487580013522845895 | NaN | lovely | 0.56 | 429681830 | 49e8d843-adf3-428b-a2c3-fe8bc6a307c9 | . &nbsp; The dataset has 9 columns, each row contains users&#39; actions like adding items to the cart or viewing an item. At a glance, we see that our dataset needs some manipulation and cleaning. The brand column has empty values, but we will check in a minute how many empty cells are out there. Now let&#39;s see columns data type. It&#39;s important to make sure that data are given the right type so our tools don&#39;t mess up when dealing with data. . # Calling dataset info dataset.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20692840 entries, 0 to 20692839 Data columns (total 9 columns): # Column Dtype -- 0 event_time object 1 event_type object 2 product_id int64 3 category_id int64 4 category_code object 5 brand object 6 price float64 7 user_id int64 8 user_session object dtypes: float64(1), int64(3), object(5) memory usage: 1.4+ GB . Here, the event_time column is been signed as object type, which will make our life harder when we do time-based analysis. So we will convert it to DateTime data type: . # changing event_time column from object to DateTime column dataset[&#39;event_time&#39;] = pd.to_datetime(dataset[&#39;event_time&#39;]) . . &nbsp; Next we will check empty values. . print(&#39;_______empty values_______ n&#39;, dataset.isna().sum()) . . _______empty values_______ event_time 0 event_type 0 product_id 0 category_id 0 category_code 20339246 brand 8757117 price 0 user_id 0 user_session 4598 dtype: int64 . &nbsp; The category_code and brand columns are 98% and 43% empty respectively, so we will get rid of them to make my cute machine fly faster. &nbsp; . # dropping &quot;category_code&quot; and &quot;brand&quot; column dataset.drop(columns=[&#39;category_code&#39;, &#39;brand&#39;], inplace=True) . . &nbsp; Lastly, let&#39;s check if there are any outliers so that we don&#39;t fall into that old mistake. &quot;garbage in -&gt; garbage out&quot; &nbsp; &nbsp; . # counting number of events per user dataset.groupby(&#39;user_id&#39;, as_index=False).agg(number_of_events=( &#39;event_type&#39;, &#39;count&#39;)).nlargest(10, &#39;number_of_events&#39;).reset_index() . . index user_id number_of_events . 0 281608 | 527021202 | 26752 | . 1 494091 | 557616099 | 9903 | . 2 985991 | 583884978 | 9420 | . 3 81834 | 419558969 | 8283 | . 4 165553 | 476450673 | 8122 | . 5 147504 | 467810091 | 8066 | . 6 505313 | 557956487 | 7735 | . 7 377367 | 550388516 | 7556 | . 8 391614 | 552908674 | 7419 | . 9 229928 | 506877330 | 6909 | . &nbsp; The first user have an enormous number of events which could mean that it&#39;s not human or one of the employees doing their work using a user account. Possibilities are endless so we will exclude him anyway. &nbsp; &nbsp; . # users list outliers = [527021202] # excluding outliers dataset.drop(dataset[dataset[&#39;user_id&#39;].isin(outliers)].index, inplace=True) . . . &nbsp; . What are we looking for? . Good analysis comes out from good questions. For any online store, the most important thing is conversion rate. which basically means: out of all visitors to the website, how many of them actually placed orders. Therefore, we will build our analysis around that and try to find insights that can help the online store to understand their visitors&#39; behavior and make better campaigns. . What is the current conversion rate? . &quot;If you can&#39;t measure it you can&#39;t manage it&quot;.&nbsp; Knowing the current state will help the stakeholders to measure whether if they are making any difference after using these analysis insights or not. . # subsetting purchase events # I found purchases with negative values, mostly it&#39;s returned orders. purchases = dataset.query(&#39;event_type == &quot;purchase&quot; &amp; price &gt; 0&#39;) # counting the total number of orders orders_count = len(purchases[&#39;user_session&#39;].unique()) # counting number of visitors visitors_count = len(dataset[&#39;user_id&#39;].unique()) # calculating conversion rate (number of orders / number of visitors) conv_rate = round((orders_count / visitors_count),2) print(f&#39;Current conversion rate is: 033[1m %{conv_rate}&#39;) . . Current conversion rate is: %0.09 . Which is below any industry average. For more on average conversion rate by industry check out this informative post. . . &nbsp; . When do users visit the store? . Finding when users visit the store is crucial for digital marketing teams in many ways. The team can adjust their campaigns during these times which will lead to better targeting new customers and reduce the cost of their campaigns. In general, we want to know when users visit the store, and whether their behavior changes during weekends or not. . Most active day of the week . # Create &#39;date&#39; column dataset[&#39;date&#39;] = dataset[&#39;event_time&#39;].dt.date # unique users in each day unieq_users = dataset.drop_duplicates(subset=[&#39;user_id&#39;, &#39;date&#39;]).copy() # Create &#39;dayname&#39; column to group by it unieq_users[&#39;dayname&#39;] = unieq_users[&#39;event_time&#39;].dt.day_name() #aggregating data count_vistors = unieq_users[&#39;dayname&#39;].value_counts() #visualizing data days_order = [&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;, &quot;Sunday&quot;] fig = px.bar(count_vistors, x=count_vistors.index, y=count_vistors.values, template=&#39;plotly_white&#39;) fig.update_layout(xaxis={&#39;title&#39;:&#39;&#39;, &#39;categoryorder&#39;:&#39;array&#39;, &#39;categoryarray&#39;:days_order}, yaxis={&#39;title&#39;:&#39;Number of visitors&#39;}, bargap=0.4) fig.show() . . Suprisengly, the most active days are the weekdays, not the weekends. . What are the peak times? . Let&#39;s be more specific and find out when do users visit the store during the day. we&#39;re going to split weekdays from the weekends since weekends tend to have different routines and potentially dissimilar behavior. . # Creating hour and &#39;day of week&#39; column dataset[&#39;Hour&#39;] = dataset[&#39;event_time&#39;].dt.hour dataset[&#39;d_of_week&#39;] = dataset[&#39;event_time&#39;].dt.weekday # tagging each event as &#39;Weekend&#39; or &#39;Weekday&#39; time wends_wdays = [] for ind, row in dataset.iterrows(): if row[&#39;d_of_week&#39;] &lt; 5: wends_wdays.append(&#39;Weekday&#39;) else: wends_wdays.append(&#39;Weekend&#39;) dataset[&#39;Wend/Wday&#39;] = wends_wdays . . Now we visualize peak times. . # count the number of users per hour peak_times = dataset.groupby([&#39;Hour&#39;, &#39;Wend/Wday&#39;], as_index=False).agg( {&#39;user_id&#39;: pd.Series.nunique}).rename(columns={&#39;user_id&#39;: &#39;Number of visitors&#39;}) # plotting data fig2 = px.line(peak_times, x=&#39;Hour&#39;, y=&#39;Number of visitors&#39;, color=&#39;Wend/Wday&#39;, markers=True, symbol=&#39;Wend/Wday&#39;, template=&#39;plotly_white&#39;) fig2.show() . . We have slightly different behavior between weekends and weekdays, where weekdays have two peaks during the day, but both lines drop dramatically after 8 pm. . . Top 10 products . One of the ways to raise the conversion rate is to expand the range of the products. A key insight to that is to find top selling products and find expandable ones. Now we will detect the top 10 products and their categories. . # count top 10 products top_products = purchases.groupby(&#39;product_id&#39;, as_index=False)[ &#39;event_type&#39;].count().nlargest(10, &#39;event_type&#39;) # subsetting &#39;product_id&#39; &amp; &#39;category_id&#39; columns prod_cat = purchases[[ &#39;product_id&#39;, &#39;category_id&#39;]].drop_duplicates(&#39;product_id&#39;) # selecting top 10 products with their category top_10 = prod_cat[prod_cat[&#39;product_id&#39;].isin( top_products[&#39;product_id&#39;])].reset_index(drop=True) top_10.index += 1 top_10 . . product_id category_id . 1 5700037 | 1487580009286598681 | . 2 5751383 | 1487580005092295511 | . 3 5751422 | 1487580005268456287 | . 4 5815662 | 1487580006317032337 | . 5 5849033 | 1487580005092295511 | . 6 5809912 | 1602943681873052386 | . 7 5854897 | 1487580009445982239 | . 8 5802432 | 1487580009286598681 | . 9 5304 | 1487580009471148064 | . 10 5809910 | 1602943681873052386 | . Out of the top 10 products, eight start with 148758 category_id, which represents the key number of the main category (ex. Food), and then the subcategory number comes after(ex. Rice). . . Average session duration . Another important KPI in ecommerce is the average session duration. a session is a group of user interactions with the website that take place within 30 minutes. sadly, the sessions of this store were not registered in the right way. many sessions lasted 151 days and others have 0-second duration as shown below. so we can not use this data in our analysis. . # extracting start time and end time of each session ses_st_end = dataset.groupby(&#39;user_session&#39;, as_index=False).agg( min_=(&#39;event_time&#39;, np.min), max_=(&#39;event_time&#39;, np.max)) # subtract end from start to get session duration ses_st_end[&#39;session_duration&#39;] = ses_st_end[&#39;max_&#39;] - ses_st_end[&#39;min_&#39;] # select only needed columns ses_duration = ses_st_end.loc[:, [&#39;user_session&#39;, &#39;session_duration&#39;]].sort_values( &#39;session_duration&#39;, ascending=False).reset_index(drop=True) ses_duration . . user_session session_duration . 0 ae74cec4-ae31-4470-8484-84c3a75365d3 | 151 days 14:45:56 | . 1 beac319a-88e8-43db-98e9-d6cd6184f444 | 151 days 11:10:49 | . 2 099fefe4-a74c-4dae-b9c2-fe15dea34ff1 | 151 days 10:39:38 | . 3 5b9bcf07-5c80-4f98-84dd-cad0883e0477 | 151 days 09:40:47 | . 4 285e8547-29b3-49d2-b503-5ca9a60413cc | 151 days 05:47:39 | . ... ... | ... | . 4518533 738a61cc-925e-48c8-ba6b-0ad258104504 | 0 days 00:00:00 | . 4518534 738a6219-3a52-4915-a54b-2a75d3624a88 | 0 days 00:00:00 | . 4518535 738a63f4-da1c-4f7b-8492-e2269d4f7c43 | 0 days 00:00:00 | . 4518536 738a6949-bd26-4f23-8b26-7272f36eb3da | 0 days 00:00:00 | . 4518537 0000061d-f3e9-484b-8c73-e54f355032a3 | 0 days 00:00:00 | . 4518538 rows × 2 columns . . Conclusions . The conversion rate tells us a lot about the store&#39;s performance. With a few little changes, the store can raise it without any additional costs. One of the ways is to create urgency feeling by using quantity countdown so the user act fast. Also, adjusting campaigns during peak times on Sunday, Monday, and Tuesday will bring more visits to the store, either new visitors or retargeting current users. The store can focus on top-selling categories and reduce others while testing new categories in small quantities. Data quality has some issues and the store data need to do some enhancements to have high-quality data in order to make better decisions in the future and a better understanding of visitors&#39; behavior. .",
            "url": "https://saaleh2.github.io/ALHODAIF-Portfolio/pandas/eda/2021/11/23/ecommerce-EDA-analysis.html",
            "relUrl": "/pandas/eda/2021/11/23/ecommerce-EDA-analysis.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://saaleh2.github.io/ALHODAIF-Portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m an entrepreneur turned data analyst. During my entrepreneurship, I learned priceless lessons and build up many skills to run up the business. My primary role was to overcome business challenges by collecting data, analyzing them, and applying possible solutions. .",
          "url": "https://saaleh2.github.io/ALHODAIF-Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://saaleh2.github.io/ALHODAIF-Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}